







.version 7.6
.target sm_75
.address_size 64



.visible .entry _Z12wmma_exampleP6__halfS0_Pfiiiff(
.param .u64 _Z12wmma_exampleP6__halfS0_Pfiiiff_param_0,
.param .u64 _Z12wmma_exampleP6__halfS0_Pfiiiff_param_1,
.param .u64 _Z12wmma_exampleP6__halfS0_Pfiiiff_param_2,
.param .u32 _Z12wmma_exampleP6__halfS0_Pfiiiff_param_3,
.param .u32 _Z12wmma_exampleP6__halfS0_Pfiiiff_param_4,
.param .u32 _Z12wmma_exampleP6__halfS0_Pfiiiff_param_5,
.param .f32 _Z12wmma_exampleP6__halfS0_Pfiiiff_param_6,
.param .f32 _Z12wmma_exampleP6__halfS0_Pfiiiff_param_7
)
{
.reg .pred %p<4>;
.reg .f32 %f<44>;
.reg .b32 %r<51>;
.reg .b64 %rd<16>;


ld.param.u64 %rd2, [_Z12wmma_exampleP6__halfS0_Pfiiiff_param_0];
ld.param.u64 %rd3, [_Z12wmma_exampleP6__halfS0_Pfiiiff_param_1];
ld.param.u64 %rd1, [_Z12wmma_exampleP6__halfS0_Pfiiiff_param_2];
ld.param.u32 %r3, [_Z12wmma_exampleP6__halfS0_Pfiiiff_param_3];
ld.param.u32 %r4, [_Z12wmma_exampleP6__halfS0_Pfiiiff_param_4];
ld.param.u32 %r5, [_Z12wmma_exampleP6__halfS0_Pfiiiff_param_5];
ld.param.f32 %f9, [_Z12wmma_exampleP6__halfS0_Pfiiiff_param_6];
ld.param.f32 %f10, [_Z12wmma_exampleP6__halfS0_Pfiiiff_param_7];
cvta.to.global.u64 %rd4, %rd3;
mov.u32 %r6, %ntid.x;
mov.u32 %r7, %ctaid.x;
mov.u32 %r8, %tid.x;
mad.lo.s32 %r9, %r7, %r6, %r8;
mov.u32 %r10, %ntid.y;
mov.u32 %r11, %ctaid.y;
mov.u32 %r12, WARP_SZ;
div.u32 %r13, %r9, %r12;
mov.u32 %r14, %tid.y;
mad.lo.s32 %r15, %r11, %r10, %r14;
shl.b32 %r1, %r13, 4;
cvta.to.global.u64 %rd5, %rd2;
mul.wide.s32 %rd6, %r1, 2;
add.s64 %rd7, %rd5, %rd6;
wmma.load.a.sync.aligned.col.m16n16k16.global.f16 {%r16, %r17, %r18, %r19, %r20, %r21, %r22, %r23}, [%rd7], %r3;
shl.b32 %r2, %r15, 4;
mul.lo.s32 %r24, %r2, %r5;
mul.wide.s32 %rd8, %r24, 2;
add.s64 %rd9, %rd4, %rd8;
wmma.load.b.sync.aligned.col.m16n16k16.global.f16 {%r25, %r26, %r27, %r28, %r29, %r30, %r31, %r32}, [%rd9], %r5;
shl.b32 %r33, %r3, 4;
mul.wide.s32 %rd10, %r33, 2;
add.s64 %rd11, %rd7, %rd10;
wmma.load.a.sync.aligned.col.m16n16k16.global.f16 {%r34, %r35, %r36, %r37, %r38, %r39, %r40, %r41}, [%rd11], %r3;
mov.f32 %f11, 0f00000000;
wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f12, %f13, %f14, %f15, %f16, %f17, %f18, %f19}, {%r16, %r17, %r18, %r19, %r20, %r21, %r22, %r23}, {%r25, %r26, %r27, %r28, %r29, %r30, %r31, %r32}, {%f11, %f11, %f11, %f11, %f11, %f11, %f11, %f11};
add.s64 %rd12, %rd9, 32;
wmma.load.b.sync.aligned.col.m16n16k16.global.f16 {%r42, %r43, %r44, %r45, %r46, %r47, %r48, %r49}, [%rd12], %r5;
wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f1, %f2, %f3, %f4, %f5, %f6, %f7, %f8}, {%r34, %r35, %r36, %r37, %r38, %r39, %r40, %r41}, {%r42, %r43, %r44, %r45, %r46, %r47, %r48, %r49}, {%f12, %f13, %f14, %f15, %f16, %f17, %f18, %f19};
setp.ge.s32 %p1, %r1, %r3;
setp.ge.s32 %p2, %r2, %r4;
or.pred %p3, %p2, %p1;
@%p3 bra $L__BB0_2;

mad.lo.s32 %r50, %r2, %r3, %r1;
cvta.to.global.u64 %rd13, %rd1;
mul.wide.s32 %rd14, %r50, 4;
add.s64 %rd15, %rd13, %rd14;
wmma.load.c.sync.aligned.col.m16n16k16.global.f32 {%f20, %f21, %f22, %f23, %f24, %f25, %f26, %f27}, [%rd15], %r3;
mul.f32 %f28, %f20, %f10;
fma.rn.f32 %f29, %f1, %f9, %f28;
mul.f32 %f30, %f21, %f10;
fma.rn.f32 %f31, %f2, %f9, %f30;
mul.f32 %f32, %f22, %f10;
fma.rn.f32 %f33, %f3, %f9, %f32;
mul.f32 %f34, %f23, %f10;
fma.rn.f32 %f35, %f4, %f9, %f34;
mul.f32 %f36, %f24, %f10;
fma.rn.f32 %f37, %f5, %f9, %f36;
mul.f32 %f38, %f25, %f10;
fma.rn.f32 %f39, %f6, %f9, %f38;
mul.f32 %f40, %f26, %f10;
fma.rn.f32 %f41, %f7, %f9, %f40;
mul.f32 %f42, %f27, %f10;
fma.rn.f32 %f43, %f8, %f9, %f42;
wmma.store.d.sync.aligned.col.m16n16k16.global.f32 [%rd15], {%f29, %f31, %f33, %f35, %f37, %f39, %f41, %f43}, %r3;

$L__BB0_2:
ret;

}

.visible .entry _Z17convertFp32ToFp16P6__halfPfi(
.param .u64 _Z17convertFp32ToFp16P6__halfPfi_param_0,
.param .u64 _Z17convertFp32ToFp16P6__halfPfi_param_1,
.param .u32 _Z17convertFp32ToFp16P6__halfPfi_param_2
)
{
.reg .pred %p<2>;
.reg .b16 %rs<2>;
.reg .f32 %f<2>;
.reg .b32 %r<6>;
.reg .b64 %rd<9>;


ld.param.u64 %rd1, [_Z17convertFp32ToFp16P6__halfPfi_param_0];
ld.param.u64 %rd2, [_Z17convertFp32ToFp16P6__halfPfi_param_1];
ld.param.u32 %r2, [_Z17convertFp32ToFp16P6__halfPfi_param_2];
mov.u32 %r3, %ntid.x;
mov.u32 %r4, %ctaid.x;
mov.u32 %r5, %tid.x;
mad.lo.s32 %r1, %r3, %r4, %r5;
setp.ge.s32 %p1, %r1, %r2;
@%p1 bra $L__BB1_2;

cvta.to.global.u64 %rd3, %rd2;
mul.wide.s32 %rd4, %r1, 4;
add.s64 %rd5, %rd3, %rd4;
ld.global.f32 %f1, [%rd5];

	{ cvt.rn.f16.f32 %rs1, %f1;}


	cvta.to.global.u64 %rd6, %rd1;
mul.wide.s32 %rd7, %r1, 2;
add.s64 %rd8, %rd6, %rd7;
st.global.u16 [%rd8], %rs1;

$L__BB1_2:
ret;

}

